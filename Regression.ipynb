{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cc94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# I. DATA LOADING AND PREPARATION\n",
    "# =========================================================\n",
    "# Attempt to load the file, handling potential path/index issues from previous runs\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "try:\n",
    "    # Assuming the structure from the last successful read (Median_House_Value as a column)\n",
    "    california_houses = pd.read_csv(\"datasets/California_Houses.csv\")\n",
    "except FileNotFoundError:\n",
    "    california_houses = pd.read_csv(\"California_Houses.csv\")\n",
    "\n",
    "# Separate features (X) and target (T)\n",
    "X = california_houses.drop(columns=['Median_House_Value'])\n",
    "T = california_houses['Median_House_Value']\n",
    "\n",
    "\n",
    "# Shuffle the data to ensure randomness\n",
    "shuffled_data = pd.concat([X, T], axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X_shuffled = shuffled_data.drop(columns=['Median_House_Value'])\n",
    "T_shuffled = shuffled_data['Median_House_Value']\n",
    "\n",
    "# Define the split points (70% train, 15% validation, 15% test)\n",
    "total_rows = X_shuffled.shape[0]\n",
    "train_end = int(total_rows * 0.7)\n",
    "validation_end = int(total_rows * 0.85)\n",
    "\n",
    "# Assign the training data portion (0% to 70%)\n",
    "X_train_raw = X_shuffled.iloc[:train_end]\n",
    "T_train = T_shuffled.iloc[:train_end]\n",
    "\n",
    "# Assign the validation data portion (70% to 85%)\n",
    "X_validation_raw = X_shuffled.iloc[train_end:validation_end]\n",
    "T_validation = T_shuffled.iloc[train_end:validation_end]\n",
    "\n",
    "# Assign the test data portion (85% to 100%)\n",
    "X_test_raw = X_shuffled.iloc[validation_end:]\n",
    "T_test = T_shuffled.iloc[validation_end:]\n",
    "\n",
    "\n",
    "# --- Feature Scaling (Crucial for Gradient Descent) ---\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "# Use the SAME fitted scaler to transform the validation and test data\n",
    "X_validation_scaled = scaler.transform(X_validation_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "\n",
    "# --- Add Bias Term ---\n",
    "\n",
    "# Add bias term (column of ones) to all three scaled sets\n",
    "X_train_b_scaled = np.c_[np.ones((len(X_train_scaled), 1)), X_train_scaled]\n",
    "X_validation_b_scaled = np.c_[np.ones((len(X_validation_scaled), 1)), X_validation_scaled]\n",
    "X_test_b_scaled = np.c_[np.ones((len(X_test_scaled), 1)), X_test_scaled]\n",
    "\n",
    "\n",
    "# --- Reshape Target Variables ---\n",
    "\n",
    "# Reshape target variables into column vectors\n",
    "T_train_col = T_train.values.reshape(-1, 1)\n",
    "T_validation_col = T_validation.values.reshape(-1, 1)\n",
    "T_test_col = T_test.values.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96208280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "             1. DIRECT SOLUTION (NORMAL EQUATION)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# II. DIRECT SOLUTION (NORMAL EQUATION)\n",
    "# =========================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"             1. DIRECT SOLUTION (NORMAL EQUATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 1. Train (Using UN-SCALED features for the Normal Equation) ---\n",
    "# The Normal Equation is not sensitive to feature scaling.\n",
    "X_train_b_raw = np.c_[np.ones((len(X_train_raw), 1)), X_train_raw.values]\n",
    "\n",
    "# W* = (X_T * X)^-1 * X_T * T\n",
    "W_normal = np.linalg.pinv(X_train_scaled.T @ X_train_scaled) @ X_train_scaled.T @ T_train_col\n",
    "\n",
    "# --- 2. Predict on Test Set ---\n",
    "X_test_b_raw = np.c_[np.ones((len(X_test_raw), 1)), X_test_raw.values]\n",
    "T_validation_predict_normal = X_validation_scaled @ W_normal\n",
    "T_test_predict_normal = X_train_scaled @ W_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285f2806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "             2. GRADIENT DESCENT (BATCH GD)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# III. GRADIENT DESCENT (BATCH GD)\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"             2. GRADIENT DESCENT (BATCH GD)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iterations=2000):\n",
    "    m = len(y)\n",
    "    # Initialize theta (coefficients) to zeros\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Calculate Error\n",
    "        error = (X @ theta) - y\n",
    "        \n",
    "        # Calculate Gradient (Vectorized Partial Derivative)\n",
    "        gradients = (1/m) * X.T @ error\n",
    "\n",
    "        # Update theta\n",
    "        theta = theta - learning_rate * gradients\n",
    "\n",
    "    return theta\n",
    "\n",
    "# --- 1. Train (Using SCALED features) ---\n",
    "learning_rate = 0.01\n",
    "n_iterations = 2000\n",
    "W_gd = gradient_descent(X_train_b_scaled, T_train_col, learning_rate, n_iterations)\n",
    "\n",
    "# --- 2. Predict on Test Set ---\n",
    "# T_validation_predict_gd = X_validation_b_scaled @ W_gd\n",
    "T_test_predict_gd = X_test_b_scaled @ W_gd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3cb6cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65111/1007270515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# --- 1. Compare Coefficients ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Intercept'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m coefficients_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;34m'Feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m'Normal Eq. (Raw Scale)'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mW_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# IV. COMPARISON AND EVALUATION\n",
    "# =========================================================\n",
    "\n",
    "def compare_metrics(T_true, T_pred_normal, T_pred_gd, model_type):\n",
    "    \"\"\"Calculates and compares metrics for both models on a given set.\"\"\"\n",
    "    mse_normal = mean_squared_error(T_true, T_pred_normal)\n",
    "    rmse_normal = np.sqrt(mse_normal)\n",
    "    r2_normal = r2_score(T_true, T_pred_normal)\n",
    "    \n",
    "    mse_gd = mean_squared_error(T_true, T_pred_gd)\n",
    "    rmse_gd = np.sqrt(mse_gd)\n",
    "    r2_gd = r2_score(T_true, T_pred_gd)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['RMSE', 'R-squared ($R^2$)'],\n",
    "        'Normal Equation': [f'{rmse_normal:,.2f}', f'{r2_normal:.4f}'],\n",
    "        'Gradient Descent': [f'{rmse_gd:,.2f}', f'{r2_gd:.4f}']\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n--- Model Performance Comparison on the {model_type} ---\")\n",
    "    # print(metrics_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# --- 1. Compare Coefficients ---\n",
    "feature_names = ['Intercept'] + list(X_train_raw.columns)\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Normal Eq. (Raw Scale)': W_normal.flatten(),\n",
    "    'GD (Scaled)': W_gd.flatten()\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"           COEFFICIENT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: Normal Eq. weights apply to raw features, GD weights to scaled features.\")\n",
    "# print(coefficients_df.to_markdown(floatfmt=\".4f\"))\n",
    "\n",
    "\n",
    "# --- 2. Compare Metrics on Test Set ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"         TEST SET METRICS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "com = compare_metrics(T_test_col, T_test_predict_normal, T_test_predict_gd, \"Test Set\")\n",
    "coefficients_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
