{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# III. GRADIENT DESCENT (BATCH GD) - REGULARIZED VERSIONS\n",
    "\n",
    "\n",
    "# Helper function to calculate Mean Squared Error\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def ridge_gradient_descent(X, y, lambda_reg, learning_rate=0.01, n_iterations=5000):\n",
    "    m, n = X.shape\n",
    "    # Initialize theta (coefficients)\n",
    "    theta = np.zeros((n, 1))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        predictions = X @ theta\n",
    "        error = predictions - y\n",
    "        \n",
    "        # Standard Gradient (1/m * X.T @ error)\n",
    "        gradients = (1/m) * X.T @ error\n",
    "\n",
    "        # L2 Regularization Penalty Gradient (2*lambda*w)\n",
    "        # We create a penalty vector: weights * 2 * lambda_reg\n",
    "        # Note: Do NOT penalize the intercept/bias term (index 0)\n",
    "        l2_penalty_gradient = (2 * lambda_reg / m) * theta\n",
    "        l2_penalty_gradient[0] = 0 # Set penalty for bias term to zero\n",
    "\n",
    "        # Update theta with the combined gradient\n",
    "        theta = theta - learning_rate * (gradients + l2_penalty_gradient)\n",
    "        \n",
    "    return theta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c248841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso regression\n",
    "def lasso_gradient_descent(X, y, lambda_reg, learning_rate=0.01, n_iterations=5000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        predictions = X @ theta\n",
    "        error = predictions - y\n",
    "        \n",
    "        # Standard Gradient (1/m * X.T @ error)\n",
    "        gradients = (1/m) * X.T @ error\n",
    "\n",
    "        # L1 Regularization Penalty Gradient (lambda * sign(w))\n",
    "        # We create a penalty vector: lambda * sign(weights)\n",
    "        # Note: Do NOT penalize the intercept/bias term (index 0)\n",
    "        l1_penalty_gradient = (lambda_reg / m) * np.sign(theta)\n",
    "        l1_penalty_gradient[0] = 0 # Set penalty for bias term to zero\n",
    "        \n",
    "        # Update theta with the combined gradient\n",
    "        theta = theta - learning_rate * (gradients + l1_penalty_gradient)\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f70609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================================\n",
    "# V. REGULARIZATION TUNING & PLOTTING (Manual Implementation)\n",
    "# =========================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  REGULARIZATION TUNING (Ridge and Lasso)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define a range of lambda values on a log scale for tuning\n",
    "lambda_values = np.logspace(-4, 4, 30) # 30 points from 10^-4 to 10^4\n",
    "\n",
    "# Set training parameters\n",
    "# Note: You may need to tune learning_rate or n_iterations if models don't converge\n",
    "learning_rate = 0.01\n",
    "n_iterations = 5000\n",
    "\n",
    "ridge_validation_errors = []\n",
    "lasso_validation_errors = []\n",
    "\n",
    "# --- 1. RIDGE REGRESSION TUNING ---\n",
    "for lambda_val in lambda_values:\n",
    "    # Train the model on the Training Set\n",
    "    ridge_weights = ridge_gradient_descent(\n",
    "        X_train_b_scaled, T_train_col, \n",
    "        lambda_reg=lambda_val,\n",
    "        learning_rate=learning_rate,\n",
    "        n_iterations=n_iterations\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate on the Validation Set\n",
    "    T_validation_predict_ridge = X_validation_b_scaled @ ridge_weights\n",
    "    mse_val = calculate_mse(T_validation_col, T_validation_predict_ridge)\n",
    "    ridge_validation_errors.append(mse_val)\n",
    "\n",
    "# --- 2. LASSO REGRESSION TUNING ---\n",
    "for lambda_val in lambda_values:\n",
    "    # Train the model on the Training Set\n",
    "    lasso_weights = lasso_gradient_descent(\n",
    "        X_train_b_scaled, T_train_col, \n",
    "        lambda_reg=lambda_val,\n",
    "        learning_rate=learning_rate,\n",
    "        n_iterations=n_iterations\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate on the Validation Set\n",
    "    T_validation_predict_lasso = X_validation_b_scaled @ lasso_weights\n",
    "    mse_val = calculate_mse(T_validation_col, T_validation_predict_lasso)\n",
    "    lasso_validation_errors.append(mse_val)\n",
    "\n",
    "# Find optimal lambda\n",
    "optimal_lambda_ridge = lambda_values[np.argmin(ridge_validation_errors)]\n",
    "optimal_lambda_lasso = lambda_values[np.argmin(lasso_validation_errors)]\n",
    "\n",
    "print(f\"Optimal Lambda for Ridge: {optimal_lambda_ridge:.4e} (Min Validation MSE: {np.min(ridge_validation_errors):,.2f})\")\n",
    "print(f\"Optimal Lambda for Lasso: {optimal_lambda_lasso:.4e} (Min Validation MSE: {np.min(lasso_validation_errors):,.2f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
